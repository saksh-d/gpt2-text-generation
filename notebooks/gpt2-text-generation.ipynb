{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880e6121",
   "metadata": {},
   "source": [
    "# GPT-2 Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cceef8",
   "metadata": {},
   "source": [
    "## Dataset: Tiny Shakespeare\n",
    "\n",
    "Available at: https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b658d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeec275",
   "metadata": {},
   "source": [
    "### Download dataset and preview it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6210c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded and saved 'input.txt'\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\"\n",
    "filename = \"input.txt\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    print(f\"Downloaded and saved '{filename}'\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error downloading file: {e}\")\n",
    "except IOError as e:\n",
    "    print(f\"Error saving file: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7844ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read the file and preview contents\n",
    "with open('../data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Number of characters: {len(text)}\")\n",
    "\n",
    "#Print first 1000 characters\n",
    "print(text[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afd02301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "#Create vocabulary\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180a284b",
   "metadata": {},
   "source": [
    "## Tokenize the input text\n",
    "\n",
    "Build the tokenizer: Convert the raw text to some sequence of integers, and in this case, each character will be tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351ff07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 6, 1, 61, 53, 56, 50, 42, 2]\n",
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "#Create mapping from characters to integers\n",
    "\n",
    "stoi = { ch:i for i, ch in enumerate(chars)}\n",
    "itos = { i:ch for i, ch in enumerate(chars)}\n",
    "\n",
    "#building the encoder and decoder\n",
    "encode = lambda x: [stoi[c] for c in x]  #take a string, convert to integers\n",
    "decode = lambda y: ''.join([itos[i] for i in y])  #take a list of integers, convert to string\n",
    "\n",
    "print(encode(\"Hello, world!\"))\n",
    "print(decode(encode(\"Hello, world!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf9d956",
   "metadata": {},
   "source": [
    "### Encode the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3343b04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.int64) #or torch.long\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d891c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform train/test split\n",
    "\n",
    "n = int(0.9*len(data)) #90-10 split for train-val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ed6b7",
   "metadata": {},
   "source": [
    "### Context Length/Block size\n",
    "\n",
    "The data is sampled in chunks, and the length of the chunk of text that is input to the model is defined as \"context length\" or \"block size\".\n",
    "\n",
    "The transformer sees everything from one character up to the block size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7088bb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]) the target is: 47\n",
      "When input is tensor([18, 47]) the target is: 56\n",
      "When input is tensor([18, 47, 56]) the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]) the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]) the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]) the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is: 58\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]  #+1, as the target for each position is the next character\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]  #offset by 1\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]  #t characters including the t'th character\n",
    "    target = y[t]  #t+1'th character\n",
    "    print(f\"When input is {context} the target is: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaadab6b",
   "metadata": {},
   "source": [
    "### Batches\n",
    "For parallel processing of data, batches of chunks are processed at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ff3d351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:  torch.Size([4, 8]) tensor([[41, 53, 56, 52, 57,  1, 58, 46],\n",
      "        [ 1, 47, 44,  1, 21,  1, 40, 43],\n",
      "        [56,  5, 42,  1, 58, 46, 43,  1],\n",
      "        [51,  6,  1, 59, 52, 42, 56, 43]])\n",
      "targets:  torch.Size([4, 8]) tensor([[53, 56, 52, 57,  1, 58, 46, 43],\n",
      "        [47, 44,  1, 21,  1, 40, 43,  0],\n",
      "        [ 5, 42,  1, 58, 46, 43,  1, 51],\n",
      "        [ 6,  1, 59, 52, 42, 56, 43, 57]])\n",
      "When input is [41], the target is 53\n",
      "When input is [41, 53], the target is 56\n",
      "When input is [41, 53, 56], the target is 52\n",
      "When input is [41, 53, 56, 52], the target is 57\n",
      "When input is [41, 53, 56, 52, 57], the target is 1\n",
      "When input is [41, 53, 56, 52, 57, 1], the target is 58\n",
      "When input is [41, 53, 56, 52, 57, 1, 58], the target is 46\n",
      "When input is [41, 53, 56, 52, 57, 1, 58, 46], the target is 43\n",
      "When input is [1], the target is 47\n",
      "When input is [1, 47], the target is 44\n",
      "When input is [1, 47, 44], the target is 1\n",
      "When input is [1, 47, 44, 1], the target is 21\n",
      "When input is [1, 47, 44, 1, 21], the target is 1\n",
      "When input is [1, 47, 44, 1, 21, 1], the target is 40\n",
      "When input is [1, 47, 44, 1, 21, 1, 40], the target is 43\n",
      "When input is [1, 47, 44, 1, 21, 1, 40, 43], the target is 0\n",
      "When input is [56], the target is 5\n",
      "When input is [56, 5], the target is 42\n",
      "When input is [56, 5, 42], the target is 1\n",
      "When input is [56, 5, 42, 1], the target is 58\n",
      "When input is [56, 5, 42, 1, 58], the target is 46\n",
      "When input is [56, 5, 42, 1, 58, 46], the target is 43\n",
      "When input is [56, 5, 42, 1, 58, 46, 43], the target is 1\n",
      "When input is [56, 5, 42, 1, 58, 46, 43, 1], the target is 51\n",
      "When input is [51], the target is 6\n",
      "When input is [51, 6], the target is 1\n",
      "When input is [51, 6, 1], the target is 59\n",
      "When input is [51, 6, 1, 59], the target is 52\n",
      "When input is [51, 6, 1, 59, 52], the target is 42\n",
      "When input is [51, 6, 1, 59, 52, 42], the target is 56\n",
      "When input is [51, 6, 1, 59, 52, 42, 56], the target is 43\n",
      "When input is [51, 6, 1, 59, 52, 42, 56, 43], the target is 57\n"
     ]
    }
   ],
   "source": [
    "#looking at one batch of size (batch_size, block_size)\n",
    "\n",
    "batch_size = 4 #how many independent sequences will be processed in parallel\n",
    "block_size = 8 #maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))  #generate random positions to grab chunk out of  | torch.randint(low, high, (size:tuple))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  #offset by 1\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs: ', xb.shape, xb)\n",
    "print('targets: ', yb.shape, yb)\n",
    "\n",
    "for b in range(batch_size):   #batch dimension\n",
    "    for t in range(block_size):   #time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"When input is {context.tolist()}, the target is {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ea3a8",
   "metadata": {},
   "source": [
    "## Bigram Language Model\n",
    "\n",
    "Refer to 'bigram-language-model.ipynb' for more on this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bc97a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65]) tensor(4.5823, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "OWzN'Cx.ZdBfSgRwBEmOaurXvCA, W!Sy\n",
      "QQ.A-otYOWm.MuZtHvtBCTHdM:cPwfW\n",
      " cPhSRx\n",
      "hdQkjwfOwWmTAZmLEid W!jFRx\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) #logits for the next token are stored in a lookup table\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)  #idx and targets are both (Batch, Time) shaped tensor of ints. Logits has size (Batch, Time, Channels=vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #Pytorch expects (B,C,T) as opposed to the (B,T,C) we created\n",
    "            B, T, C = logits.shape #unpacking\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T) #these are originally (B,T)\n",
    "            #cross_entropy = negative average log likelihood\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss #scores for the next character based on the singular input token\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            #focus only on last time step, the last character\n",
    "            logits = logits[:, -1, :] #becomes (B,C) from (B,T,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            #sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
    "            #append to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B,T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out, loss = m(xb, yb)\n",
    "print(out.shape, loss)\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3301ef46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.607495069503784\n",
      "\n",
      "Tyme.\n",
      "Lo,\n",
      "O:\n",
      "Har toshis marnd l oftoves tr t wnk, weree J&wat'ENCoulllero? larvem d secor woour winukitoue t heincebery R:\n",
      "NG f ans, y arkend chaivio t whe handst, bare ngess IO:\n",
      "Fant; f nde sp\n",
      "Thourghal wnd\n",
      "\n",
      "As h\n",
      "I s lichinde,\n",
      "ARCENIULINThe pshecthatf indathe? cP$S:\n",
      "UD t atl I y g,\n",
      "Maferrde IOfore burd. w t.\n",
      "noot ter an'The Yotunss t tatomen, arllI YOLULOLakONGOns,\n",
      "Awd py sJol tweforece s metthocantothiffarthereand s s a Ifftan Prr fty f d, s orZAtukes we G filotherop:\n",
      "\n",
      "\n",
      "Sh, nd I yodeg ME:\n",
      "ARDY\n"
     ]
    }
   ],
   "source": [
    "#Optimizer: Takes gradients and updates parameters\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72753a4",
   "metadata": {},
   "source": [
    "This is the simplest possible model. Clearly the results show that it is not generating the expected text, but there is a semblance of Shakespearean text in this output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d85b944",
   "metadata": {},
   "source": [
    "### Towards self attention\n",
    "\n",
    "Toy example to understand self attention.\n",
    "\n",
    "A token at the nth location should only be considering information from all previous tokens and not future tokens, since those are the ones to be predicted.\n",
    "\n",
    "We need to allow a token to interact with its preceeding elements. calculate the average of all the previous tokens and the current token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a6c02f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2265e-03, -2.8304e+00],\n",
      "        [ 4.4351e-01, -6.9117e-01],\n",
      "        [ 2.0624e+00,  2.0980e+00],\n",
      "        [ 2.6734e-01, -3.6565e-01],\n",
      "        [ 8.6182e-01, -1.2233e+00],\n",
      "        [-1.1652e+00,  1.6934e-01],\n",
      "        [ 1.4138e-01,  1.9059e+00],\n",
      "        [-4.9645e-01,  8.6998e-01]]) tensor([[-1.2265e-03, -2.8304e+00],\n",
      "        [ 2.2114e-01, -1.7608e+00],\n",
      "        [ 8.3490e-01, -4.7451e-01],\n",
      "        [ 6.9301e-01, -4.4730e-01],\n",
      "        [ 7.2677e-01, -6.0251e-01],\n",
      "        [ 4.1145e-01, -4.7387e-01],\n",
      "        [ 3.7287e-01, -1.3390e-01],\n",
      "        [ 2.6420e-01, -8.4185e-03]])\n"
     ]
    }
   ],
   "source": [
    "# Toy example\n",
    "B, T, C = 4, 8, 2 #batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape\n",
    "\n",
    "xbow = torch.zeros(B,T,C) #x_bag_of_words - used for averaging things\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b,:t+1]  #(t, C)\n",
    "        xbow[b,t] = torch.mean(x_prev, dim=0)\n",
    "\n",
    "print(x[0], xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0399b67a",
   "metadata": {},
   "source": [
    "This can be made more effecient by using a lower traingular matrix multiplication with `torch.tril(torch.ones())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75f6fc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b =  tensor([[9., 7.],\n",
      "        [8., 8.],\n",
      "        [6., 6.]])\n",
      "c =  tensor([[9.0000, 7.0000],\n",
      "        [8.5000, 7.5000],\n",
      "        [7.6667, 7.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3, 3)) #lower triangular matrix of all 1's\n",
    "a = a / torch.sum(a, 1, keepdim=True)  #normalize it so that each row adds up to 1 (gives average for matmul in the next step)\n",
    "b = torch.randint(2, 10, size=(3, 2)).float()\n",
    "c = a @ b  #yields average matrix as above\n",
    "\n",
    "print(\"a = \", a)\n",
    "print(\"b = \", b)\n",
    "print(\"c = \", c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d5d763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2265e-03, -2.8304e+00],\n",
       "        [ 2.2114e-01, -1.7608e+00],\n",
       "        [ 8.3490e-01, -4.7451e-01],\n",
       "        [ 6.9301e-01, -4.4730e-01],\n",
       "        [ 7.2677e-01, -6.0251e-01],\n",
       "        [ 4.1145e-01, -4.7387e-01],\n",
       "        [ 3.7287e-01, -1.3390e-01],\n",
       "        [ 2.6420e-01, -8.4186e-03]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applying this to the toy example above\n",
    "\n",
    "weights = torch.tril(torch.ones(T,T))\n",
    "weights = weights / torch.sum(weights, dim=1, keepdim=True)\n",
    "xbow2 = weights @ x  #(B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "xbow2[0] #same as xbow from above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a007d",
   "metadata": {},
   "source": [
    "A third way to achieve this would be to use softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7b04496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2265e-03, -2.8304e+00],\n",
       "        [ 2.2114e-01, -1.7608e+00],\n",
       "        [ 8.3490e-01, -4.7451e-01],\n",
       "        [ 6.9301e-01, -4.4730e-01],\n",
       "        [ 7.2677e-01, -6.0251e-01],\n",
       "        [ 4.1145e-01, -4.7387e-01],\n",
       "        [ 3.7287e-01, -1.3390e-01],\n",
       "        [ 2.6420e-01, -8.4186e-03]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = torch.zeros((T,T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "print(weights)\n",
    "\n",
    "weights = F.softmax(weights, dim=1)\n",
    "print(weights)\n",
    "\n",
    "xbow3 = weights @ x\n",
    "xbow3[0] #yet again, same output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e4848",
   "metadata": {},
   "source": [
    "### Version 4: Self-attention\n",
    "\n",
    "Implementing a single head of self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e278239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5203, 0.4797, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0796, 0.7265, 0.1939, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0081, 0.2897, 0.3129, 0.3892, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0381, 0.2742, 0.1506, 0.4870, 0.0502, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0628, 0.0504, 0.0915, 0.0337, 0.6328, 0.1287, 0.0000, 0.0000],\n",
       "        [0.0035, 0.0289, 0.1494, 0.0994, 0.4226, 0.2691, 0.0271, 0.0000],\n",
       "        [0.0443, 0.0207, 0.1250, 0.0171, 0.0934, 0.0232, 0.5833, 0.0928]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#previous information and current information is averaged together\n",
    "#implementing a single head of self-attention\n",
    "\n",
    "B, T, C = 4, 8, 32 #batch, time, channels\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) #size = B, T, head_size=16\n",
    "q = query(x) #size = B, T, head_size=16\n",
    "v = value(x)\n",
    "#All queries dot product with all keys\n",
    "weights = q @ k.transpose(-2,-1) #size(B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# weights = torch.zeros((T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf')) #upper triangular masking\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "out = weights @ v\n",
    "\n",
    "weights[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
